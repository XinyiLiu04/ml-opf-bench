import torch
import torch.nn as nn


class DenseCoreNetwork(nn.Module):
    """
    PINNæ¨¡å‹çš„æ ¸å¿ƒç¥ç»ç½‘ç»œ - PyTorchç‰ˆæœ¬ï¼ˆé›†æˆ Slack Busï¼‰

    ç‰ˆæœ¬: v2.0 - Slack Bus Integration

    æ ¸å¿ƒæ”¹åŠ¨:
    --------
    1. Pg è¾“å‡ºç»´åº¦: n_gbus_non_slackï¼ˆåªé¢„æµ‹é Slackï¼‰
    2. å¯¹å¶å˜é‡ç»´åº¦: n_gbus_allï¼ˆæ‰€æœ‰å‘ç”µæœºï¼ŒåŒ…æ‹¬ Slackï¼‰

    è®¾è®¡åŸç†:
    --------
    è™½ç„¶ Pg åªé¢„æµ‹é Slack å‘ç”µæœºï¼Œä½†å¯¹å¶å˜é‡ï¼ˆÎ¼_gï¼‰å¿…é¡»åŒ…å«æ‰€æœ‰å‘ç”µæœºï¼š
    - Slack å‘ç”µæœºè™½ç„¶ç”±åŠŸç‡å¹³è¡¡ç¡®å®šï¼Œä½†ä»ç„¶æœ‰ Pg_min/max çº¦æŸ
    - å¦‚æœ Slack Pg è§¦ç¢°çº¦æŸï¼Œç›¸åº”çš„å¯¹å¶å˜é‡ Î¼_g åº”è¯¥éé›¶
    - KKT æ¡ä»¶è¦æ±‚æ‰€æœ‰çº¦æŸéƒ½æœ‰å¯¹åº”çš„å¯¹å¶å˜é‡

    æ”¯æŒåŠ¨æ€å±‚æ•°
    """

    def __init__(self, n_gbus_non_slack, n_gbus_all, n_line,
                 neurons_in_hidden_layers_Pg, neurons_in_hidden_layers_Lm):
        """
        åˆå§‹åŒ–æ ¸å¿ƒç½‘ç»œ

        å‚æ•°:
        ----
        n_gbus_non_slack : int
            é Slack å‘ç”µæœºæ•°é‡ï¼ˆPg è¾“å‡ºç»´åº¦ï¼‰
        n_gbus_all : int
            æ‰€æœ‰å‘ç”µæœºæ•°é‡ï¼ˆå¯¹å¶å˜é‡ç»´åº¦ï¼‰
        n_line : int
            æ”¯è·¯æ•°é‡
        neurons_in_hidden_layers_Pg : list
            Pg ç½‘ç»œçš„éšè—å±‚ç¥ç»å…ƒæ•°
        neurons_in_hidden_layers_Lm : list
            Lm ç½‘ç»œçš„éšè—å±‚ç¥ç»å…ƒæ•°
        """
        super(DenseCoreNetwork, self).__init__()

        self.n_gbus_non_slack = n_gbus_non_slack
        self.n_gbus_all = n_gbus_all
        self.n_line = n_line

        # ========== Pgç½‘ç»œçš„éšè—å±‚ (åŠ¨æ€åˆ›å»º) ==========
        pg_layers = []
        prev_size = None  # å°†åœ¨forwardä¸­åŠ¨æ€ç¡®å®š
        for i, n_units in enumerate(neurons_in_hidden_layers_Pg):
            if i == 0:
                # ç¬¬ä¸€å±‚çš„è¾“å…¥å¤§å°å°†åœ¨forwardä¸­ç¡®å®š
                self.pg_input_size = None
            else:
                pg_layers.append(nn.Linear(prev_size, n_units))
                pg_layers.append(nn.ReLU())
            prev_size = n_units

        self.pg_hidden = nn.ModuleList()
        self.pg_hidden_sizes = neurons_in_hidden_layers_Pg

        # ğŸ†• Pgè¾“å‡ºå±‚ï¼ˆåªè¾“å‡ºé Slackï¼‰
        self.pg_output = nn.Linear(prev_size, n_gbus_non_slack)

        # ========== Lmç½‘ç»œçš„éšè—å±‚ (åŠ¨æ€åˆ›å»º) ==========
        lm_layers = []
        prev_size = None
        for i, n_units in enumerate(neurons_in_hidden_layers_Lm):
            if i == 0:
                self.lm_input_size = None
            else:
                lm_layers.append(nn.Linear(prev_size, n_units))
                lm_layers.append(nn.ReLU())
            prev_size = n_units

        self.lm_hidden = nn.ModuleList()
        self.lm_hidden_sizes = neurons_in_hidden_layers_Lm

        # Lmè¾“å‡ºå±‚ (æ‹‰æ ¼æœ—æ—¥ä¹˜å­) - ç»´åº¦ä¿æŒæ‰€æœ‰å‘ç”µæœº
        self.lm_output = nn.Linear(prev_size, 1)  # Î» (ç³»ç»Ÿçº§)
        self.mu_g_up_output = nn.Linear(prev_size, n_gbus_all)  # ğŸ†• æ‰€æœ‰å‘ç”µæœº
        self.mu_g_down_output = nn.Linear(prev_size, n_gbus_all)  # ğŸ†• æ‰€æœ‰å‘ç”µæœº
        self.mu_line_up_output = nn.Linear(prev_size, n_line)
        self.mu_line_down_output = nn.Linear(prev_size, n_line)

        self._initialized = False

    def _initialize_layers(self, input_size):
        """é¦–æ¬¡forwardæ—¶åˆå§‹åŒ–ç½‘ç»œå±‚"""
        if self._initialized:
            return

        # åˆå§‹åŒ–Pgç½‘ç»œ
        pg_layers = []
        prev_size = input_size
        for n_units in self.pg_hidden_sizes:
            pg_layers.append(nn.Linear(prev_size, n_units))
            pg_layers.append(nn.ReLU())
            prev_size = n_units
        self.pg_hidden = nn.Sequential(*pg_layers)

        # åˆå§‹åŒ–Lmç½‘ç»œ
        lm_layers = []
        prev_size = input_size
        for n_units in self.lm_hidden_sizes:
            lm_layers.append(nn.Linear(prev_size, n_units))
            lm_layers.append(nn.ReLU())
            prev_size = n_units
        self.lm_hidden = nn.Sequential(*lm_layers)

        self._initialized = True

        # ç§»åŠ¨åˆ°æ­£ç¡®çš„è®¾å¤‡
        device = next(self.parameters()).device
        self.pg_hidden = self.pg_hidden.to(device)
        self.lm_hidden = self.lm_hidden.to(device)

    def forward(self, inputs):
        """
        å‰å‘ä¼ æ’­ - æ”¯æŒä»»æ„å±‚æ•°

        è¿”å›:
        ----
        pg_output : Tensor [batch, n_gbus_non_slack]
            é Slack å‘ç”µæœºå‡ºåŠ›é¢„æµ‹
        lm_output : Tensor [batch, 1]
            Î» - åŠŸç‡å¹³è¡¡å¯¹å¶å˜é‡
        mu_g_up : Tensor [batch, n_gbus_all]
            å‘ç”µæœºä¸Šé™å¯¹å¶å˜é‡ï¼ˆæ‰€æœ‰å‘ç”µæœºï¼‰
        mu_g_down : Tensor [batch, n_gbus_all]
            å‘ç”µæœºä¸‹é™å¯¹å¶å˜é‡ï¼ˆæ‰€æœ‰å‘ç”µæœºï¼‰
        mu_line_up : Tensor [batch, n_line]
            çº¿è·¯ä¸Šé™å¯¹å¶å˜é‡
        mu_line_down : Tensor [batch, n_line]
            çº¿è·¯ä¸‹é™å¯¹å¶å˜é‡
        """
        # é¦–æ¬¡è°ƒç”¨æ—¶åˆå§‹åŒ–
        if not self._initialized:
            self._initialize_layers(inputs.shape[1])

        # Pgç½‘ç»œå‰å‘ä¼ æ’­ï¼ˆğŸ†• è¾“å‡ºé Slackï¼‰
        x_pg = self.pg_hidden(inputs)
        pg_output = self.pg_output(x_pg)  # [batch, n_gbus_non_slack]

        # Lmç½‘ç»œå‰å‘ä¼ æ’­ï¼ˆç»´åº¦ä¿æŒæ‰€æœ‰å‘ç”µæœºï¼‰
        x_lm = self.lm_hidden(inputs)
        lm_output = self.lm_output(x_lm)  # [batch, 1]
        mu_g_up = self.mu_g_up_output(x_lm)  # [batch, n_gbus_all]
        mu_g_down = self.mu_g_down_output(x_lm)  # [batch, n_gbus_all]
        mu_line_up = self.mu_line_up_output(x_lm)  # [batch, n_line]
        mu_line_down = self.mu_line_down_output(x_lm)  # [batch, n_line]

        return pg_output, lm_output, mu_g_up, mu_g_down, mu_line_up, mu_line_down